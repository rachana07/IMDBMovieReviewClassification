{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rachanathota/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rachanathota/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rachanathota/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk as nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCleaning(text):\n",
    "    \n",
    "    # 1. Removing meta tags\n",
    "    clean = re.compile('<.*?>')\n",
    "    text_only = re.sub(clean, '', text)\n",
    "    \n",
    "    # 2. Removing invald characters or DATA\n",
    "    \n",
    "    only_text = re.sub(r'([\\w\\.-]+@[\\w\\.-]+\\.\\w+)','',text_only)\n",
    "    \n",
    "    validChars = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', only_text, flags=re.MULTILINE)\n",
    "    \n",
    "    clean = re.compile('<.*?>')\n",
    "    cleaned_html = re.sub(clean, '', validChars)\n",
    "    \n",
    "    # 3. Into lower case\n",
    "    words = cleaned_html.lower() \n",
    "    \n",
    "    # 4. Tokenizing\n",
    "    words1 = words.split()\n",
    "    \n",
    "    # 5. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 6. Remove stop words. Restrict length grater than 2. Lematize and Stem\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_words = ''\n",
    "    for word in words1:\n",
    "        if word not in stops and len(word) > 2:\n",
    "            lemma = str(lemmatizer.lemmatize(word))\n",
    "            stem = ps.stem(lemma)\n",
    "            cleaned_words += str(lemmatizer.lemmatize(stem))\n",
    "            cleaned_words += ' '\n",
    "    \n",
    "    # 7. Returning the processed text\n",
    "    return cleaned_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFVectorizing(train_data, test_data):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(norm = 'l2')\n",
    "    \n",
    "    train_matrix = vectorizer.fit_transform(train_data)\n",
    "    test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "    return train_matrix, test_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarities(train_matrix, test_matrix):\n",
    "    \"\"\"Takes in the entire training data and the testing data (both sparse matrices) and \n",
    "        gives the cosine similarity between the two as a numpy array.\n",
    "        Numpy arrays are fastest to work with for sorting while finding nearest neighbors\"\"\"\n",
    "    \n",
    "    cosineSimilarities = np.dot(test_matrix, np.transpose(train_matrix))\n",
    "    similarities = cosineSimilarities.toarray()\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findKNearest(similarity_vector, k):\n",
    "    \"\"\"Takes in the similarity vector (numpy array) and number of neighbors to find, to return the K Nearest Neighbors indices.\n",
    "        The input array gets sorted in descending order and the first k indices returned.\n",
    "        The argsort function has been used to preserve the indices of the training reviews so that their respective labels\n",
    "        can be easily referenced in the training labels list\"\"\"\n",
    "   \n",
    "    return np.argsort(-similarity_vector)[:k]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(nearestNeighbors, labels):\n",
    "    \"\"\"Takes in the list of K nearest Neighbors and the full training labels list, and \n",
    "        calculates the count of positive and negative reviews. \n",
    "        If positive reviews are more, then the test review is positive and vice-versa\"\"\"\n",
    "    \n",
    "    positiveReviewsCount = 0\n",
    "    negativeReviewsCount = 0\n",
    "    for neighbor in nearestNeighbors:\n",
    "        if int(labels[neighbor]) == 1:\n",
    "            positiveReviewsCount += 1\n",
    "        else:\n",
    "            negativeReviewsCount += 1\n",
    "    if positiveReviewsCount > negativeReviewsCount:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-fold\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_data = pd.read_csv('test_file.txt', sep=\"\\n\\n\", engine='python') \n",
    "with open('test_file.txt', \"r\") as fr2:\n",
    "        testFile = fr2.readlines()\n",
    "        \n",
    "train_data = pd.read_csv ('train_file.txt', sep=\"\\t\")\n",
    " \n",
    "\n",
    "cleaned_train = [];\n",
    "cleaned_test = []\n",
    "\n",
    "\n",
    "for rev in train_data.review:\n",
    "    \n",
    "    cleaned_train.append(textCleaning(rev))\n",
    "    \n",
    "for rev in testFile:\n",
    "    \n",
    "    cleaned_test.append(textCleaning(rev))\n",
    "\n",
    "    \n",
    "cleaned_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF vectorizing\n",
    "vectorizer = TfidfVectorizer(norm = 'l2')\n",
    "    \n",
    "tfidf_train= vectorizer.fit_transform(cleaned_train)\n",
    "tfidf_test = vectorizer.transform(cleaned_test)\n",
    "\n",
    "tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = findSimilarities(tfidf_train, tfidf_test)\n",
    "len(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X_train, X_test, y_train, y_test, k):\n",
    "    test_sentiments = list()\n",
    "    \n",
    "    train_matrix, test_matrix = TFIDFVectorizing(X_train, X_test)\n",
    "    similarities1 = findSimilarities(train_matrix, test_matrix)\n",
    "    sents = [];\n",
    "    correct = 0;\n",
    "    for similarity in similarities1:\n",
    "        knn = findKNearest(similarity, k)\n",
    "        prediction = predict(knn, train_data.sentiment)\n",
    "    \n",
    "       #To write to the list as +1 instead of just a 1 for positive reviews\n",
    "        if prediction == 1:\n",
    "            sents.append(1)\n",
    "        else:\n",
    "            sents.append(-1)\n",
    "    correct = 0\n",
    "    for x, y in zip(sents, y_test):\n",
    "        if x == y:\n",
    "            correct = correct + 1\n",
    "    return(correct/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_k = []\n",
    "k_range = range(300, 400)\n",
    "train1 = pd.read_csv ('train_file.txt', sep=\"\\t\")\n",
    "avg1 = []\n",
    "for k in range(300, 400, 10):\n",
    "    for train_index, test_index in kf.split(train1):\n",
    "        X_train, X_test, y_train, y_test = train1.review[train_index], train1.review[test_index],train1.sentiment[train_index], train1.sentiment[test_index]\n",
    "        scores = get_score(X_train, X_test, y_train, y_test, k)\n",
    "        scores_k.append(scores)\n",
    "    avg1.append(sum(scores_k)/len(scores_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_range = range(300, 315)\n",
    "# plot to see clearly\n",
    "plt.plot(k_range, scores_k)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass every row in the numpy array of similarities to predict the sentiment of every review\n",
    "k = 300\n",
    "final_result = []\n",
    "\n",
    "for i in similarities:\n",
    "    knn = findKNearest(i, k)\n",
    "    prediction = predict(knn, train_data.sentiment)\n",
    "    \n",
    "    #To write to the list as +1 instead of just a 1 for positive reviews\n",
    "    if prediction == 1:\n",
    "        final_result.append('+1')\n",
    "    else:\n",
    "        final_result.append('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the result to a .dat file\n",
    "output = open('output.txt', 'w')\n",
    "\n",
    "output.writelines( \"%s\\n\" % item for item in final_result )\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
